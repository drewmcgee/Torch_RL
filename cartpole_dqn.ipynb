{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ca16f-7435-4aff-bce0-b739d3dc5dae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44510800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec12eaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeeb500",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train a simple NN on a single training sample!\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 2)  # 784 input features, 128 output features\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.fc3 = nn.Linear(2, 2)\n",
    "        self.fc4 = nn.Linear(2, 1)\n",
    "        #self.fc3 = nn.Linear(2, 1)    # 10 output features for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d705fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Net\n",
    "net = Net()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Optimizer stores the weights / biases\n",
    "\n",
    "# Random data and target labels for demonstration\n",
    "inputs = torch.randn(1, 4)  # Example input\n",
    "targets = torch.tensor([0])   # Example target. know to treat this index of class 0-9 and compare to max output?\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward + backward + optimize\n",
    "outputs = net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32858cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Now lets try training on multiple samples\n",
    "\n",
    "With multiple samples you can choose how often you want to backpropagate. For example do you want to backprop after every sample or backprop after completing a batch.  Or even after a full pass through the training data (an epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f2d28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552fceb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Random data and target labels for demonstration\n",
    "inputs = torch.randn(10000, 4)  # tensor of 10 vectors of length 4.\n",
    "targets = torch.tensor([[sum(inputs[i])] for i in range(len(inputs))], dtype=torch.float32)  # target is sum of input vector\n",
    "#targets = torch.tensor([[1.1]]*10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a65ccb9",
   "metadata": {},
   "source": [
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward + backward + optimize\n",
    "outputs = net(inputs)\n",
    "#loss = criterion(outputs, targets)\n",
    "loss = F.smooth_l1_loss(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfab40a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "losses = []\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "for i in range(len(inputs) // batch_size + 1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    input_batch = inputs[i: i + batch_size]\n",
    "    target_batch = targets[i: i + batch_size]\n",
    "    \n",
    "\n",
    "    \n",
    "    output_batch = net(input_batch)\n",
    "    loss = F.mse_loss(output_batch, target_batch)\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72aa991",
   "metadata": {},
   "source": [
    "## Throwing minimal DQN at Cartpole-V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b642dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQN Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 24)  # CartPole state is 4-dimensional\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        #self.fc2_1 = nn.Linear(24, 24) \n",
    "        #self.fc2_2 = nn.Linear(24, 24)\n",
    "        #self.fc2_3 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, 2)  # Two actions: push cart left or right\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #x = torch.relu(self.fc2_1(x))\n",
    "        #x = torch.relu(self.fc2_2(x))\n",
    "        #x = torch.relu(self.fc2_3(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acb35a",
   "metadata": {},
   "source": [
    "Below we try our oversimplified DQN on Cartpole. It doesn't learn anything. I think this is because the penalty (negative reward) of the pole falling is only directly attributed to the single action before it and not the series of actions before it, which may have already guaranteed the fall. In this example, we are backpropping after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889feed8",
   "metadata": {},
   "source": [
    "I think what we need to do is rather than backprop after each step we should backprop after each episode and make sure the negative reward is visible to a window of several timesteps before the actual fall. I'm guessing this is where a replay buffer comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2377217f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize environment, model, and optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN()\n\u001b[1;32m      4\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Blur the state\n",
    "        precision = 0.001\n",
    "        state_tensor = torch.round(state_tensor / precision) * precision\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -1  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_value = (torch.tensor(reward) + gamma * \n",
    "                                       model(next_state_tensor).max(1)[0] * (1-done)).unsqueeze(0)\n",
    "\n",
    "        # Compute the loss\n",
    "        state_action_value = model(state_tensor)[0, action].unsqueeze(0)\n",
    "        loss = loss_fn(state_action_value, expected_state_action_value)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "    \n",
    "    if t >= 500:\n",
    "        print(\"breaking on 500 training\")\n",
    "        break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d032f-22e0-4976-b1af-02a9b74f3873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Model and Display on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5400709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "\n",
    "def eval_model(yo_model):\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    state = env.reset()[0]\n",
    "    timesteps = 0 \n",
    "    actions = []\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action = yo_model(state_tensor).max(1)[1].item()\n",
    "        actions.append(action)\n",
    "        timesteps += 1\n",
    "        #action = modelenv.action_space.sample()  # replace with your action selection method\n",
    "        state, reward, term,trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            #state = env.reset()[0]\n",
    "            print(\"time steps:\", timesteps)\n",
    "            break\n",
    "            \n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdb58c-f68a-43e0-8c9a-392b20304930",
   "metadata": {},
   "source": [
    "## Control Cartpole Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa69896-6c49-491c-b897-50b57b3fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\",render_mode='human')\n",
    "env.reset()\n",
    "while True:\n",
    "    action = int(input(\"Action: \"))\n",
    "    if action in (0, 1):\n",
    "        state,reward,term,trunc,info = env.step(action)\n",
    "        env.render()\n",
    "        if term or trunc:\n",
    "            break\n",
    "         \n",
    "#env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4819b83-ef9c-4e20-b3bd-3bf568717c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame and the environment\n",
    "pygame.init()\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Set up the display\n",
    "screen_width, screen_height = 600, 400\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption(\"Click Window to Start\")\n",
    "\n",
    "# Start the environment\n",
    "observation = env.reset()\n",
    "done = False\n",
    "clock = pygame.time.Clock()\n",
    "steps = 0\n",
    "\n",
    "waiting_for_click = True\n",
    "while waiting_for_click:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:  # Wait for click to start\n",
    "            waiting_for_click = False\n",
    "        elif event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()  # Exit the entire script if the window is closed\n",
    "\n",
    "\n",
    "while not done or done:\n",
    "    # Render the environment to the pygame window\n",
    "    env.render()\n",
    "\n",
    "    # Check for key presses to control the cart\n",
    "    #action = 1  # Default action (don't move)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_LEFT:\n",
    "                action = 0  # Move cart to the left\n",
    "            elif event.key == pygame.K_RIGHT:\n",
    "                action = 1  # Move cart to the right\n",
    "        elif event.type == pygame.QUIT:\n",
    "            done = True\n",
    "\n",
    "    # Step the environment with the chosen action\n",
    "    observation, reward, term,trunc, info = env.step(action)\n",
    "    steps += 1\n",
    "    done = term or trunc\n",
    "\n",
    "    # Update the display and wait a short duration\n",
    "    pygame.display.flip()\n",
    "    clock.tick(5)  # Limit to 60 frames per second\n",
    "\n",
    "print(\"your timesteps:\", steps)\n",
    "\n",
    "#env.close()\n",
    "#pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0408b4b-0920-4286-b7f8-f9e1d015340e",
   "metadata": {},
   "source": [
    "## Implement a Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdad570-bf2c-4b75-8797-c121b1fed8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()  # Set the target network to evaluation mode\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Blur the state\n",
    "        #precision = 0.001\n",
    "        #state_tensor = torch.round(state_tensor / precision) * precision\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -10  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_value = (torch.tensor(reward) + gamma * \n",
    "                                       target_model(next_state_tensor).max(1)[0] * (1-done)).unsqueeze(0)\n",
    "\n",
    "        # Compute the loss\n",
    "        state_action_value = model(state_tensor)[0, action].unsqueeze(0)\n",
    "        loss = loss_fn(state_action_value, expected_state_action_value)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "    \n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    #if t >= 500:\n",
    "    #    print(\"breaking on 500 training\")\n",
    "    #    break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f06ec-2e69-439e-a4e8-6053363c47f7",
   "metadata": {},
   "source": [
    "## Now Lets Implement a Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18aa1f5-c1ca-4ad0-9428-af2ed8dffbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049aa793-e832-4ede-bf0e-1dfb73f72ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc8d60c-e2fe-4c3e-82f7-baa033dc1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 23 timesteps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(state_q_value, expected_q_value_batch)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/CODE_LEARNING/pytorch/Q-Learning/torch-env/lib/python3.11/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/CODE_LEARNING/pytorch/Q-Learning/torch-env/lib/python3.11/site-packages/torch/_dynamo/decorators.py:47\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/CODE_LEARNING/pytorch/Q-Learning/torch-env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:307\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    304\u001b[0m on_enter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_enter\n\u001b[1;32m    305\u001b[0m backend_ctx_ctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_ctx_ctor\n\u001b[0;32m--> 307\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@functools\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwraps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m_fn\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDisableContext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symbolic_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_fx_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_on_nested_fx_trace\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/functools.py:35\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     32\u001b[0m WRAPPER_ASSIGNMENTS \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m WRAPPER_UPDATES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m,)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_wrapper\u001b[39m(wrapper,\n\u001b[1;32m     36\u001b[0m                    wrapped,\n\u001b[1;32m     37\u001b[0m                    assigned \u001b[38;5;241m=\u001b[39m WRAPPER_ASSIGNMENTS,\n\u001b[1;32m     38\u001b[0m                    updated \u001b[38;5;241m=\u001b[39m WRAPPER_UPDATES):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update a wrapper function to look like the wrapped function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m       wrapper is the function to be updated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m       function (defaults to functools.WRAPPER_UPDATES)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m assigned:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 5000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "batch_size = 50\n",
    "\n",
    "# Training Data Analysis\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    \n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -1  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        experience = (state_tensor, action, reward, next_state_tensor, done)\n",
    "        replay_buffer.push(*experience)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # Random sample from the Replay Buffer\n",
    "            experience_batch = replay_buffer.sample(batch_size)\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*experience_batch)\n",
    "\n",
    "            #print(batch_state)\n",
    "            #print(batch_action)\n",
    "            #print(batch_reward)\n",
    "            #print(batch_next_state)\n",
    "            #print(batch_done)\n",
    "\n",
    "            # Convert batches to PyTorch tensors\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_action = torch.tensor(batch_action, dtype=torch.long)  # Corrected line\n",
    "            batch_reward = torch.tensor(batch_reward, dtype=torch.float)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "            batch_done = torch.tensor(batch_done, dtype=torch.float)\n",
    "\n",
    "\n",
    "            # Model prediction of Q values  \n",
    "            state_q_value = model(batch_state)[:,0]\n",
    "            state_q_value = torch.stack([state_q_value[idx][action_idx] for idx,action_idx in enumerate(batch_action)])\n",
    "            state_q_value = state_q_value.unsqueeze(1)\n",
    "            \n",
    "            # Compute the expected Q values (Bellman Equation)\n",
    "            expected_q_value_batch = (batch_reward + \n",
    "                                      gamma * model(batch_next_state).max(2)[0].squeeze() * (1-batch_done)).unsqueeze(1)\n",
    "            \n",
    "\n",
    "            loss = loss_fn(state_q_value, expected_q_value_batch)\n",
    "    \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986201e1-134f-41b5-b1df-4c81813b5e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8025],\n",
       "        [ 3.6528],\n",
       "        [ 0.5515],\n",
       "        [ 2.9403],\n",
       "        [ 4.3641],\n",
       "        [ 4.7606],\n",
       "        [ 0.3282],\n",
       "        [ 3.5947],\n",
       "        [ 3.6855],\n",
       "        [ 3.8108],\n",
       "        [-0.5787],\n",
       "        [ 3.3134],\n",
       "        [ 1.2739],\n",
       "        [ 4.4049],\n",
       "        [ 1.9770],\n",
       "        [ 1.4646],\n",
       "        [ 2.0570],\n",
       "        [ 2.0744],\n",
       "        [ 4.1432],\n",
       "        [ 4.5526],\n",
       "        [ 3.6341],\n",
       "        [ 3.3051],\n",
       "        [ 0.1354],\n",
       "        [ 2.1898],\n",
       "        [-0.6440],\n",
       "        [ 3.2352],\n",
       "        [ 2.5514],\n",
       "        [ 3.1803],\n",
       "        [ 1.8424],\n",
       "        [ 4.2790],\n",
       "        [ 4.8002],\n",
       "        [ 4.8734],\n",
       "        [ 2.6041],\n",
       "        [ 4.4981],\n",
       "        [ 0.0271],\n",
       "        [ 3.6024],\n",
       "        [-0.7242],\n",
       "        [-0.6985],\n",
       "        [ 2.6550],\n",
       "        [ 1.6143],\n",
       "        [-0.1972],\n",
       "        [ 0.4030],\n",
       "        [ 4.3996],\n",
       "        [ 3.6417],\n",
       "        [ 3.0449],\n",
       "        [ 4.7365],\n",
       "        [ 3.9903],\n",
       "        [ 0.2311],\n",
       "        [ 4.9575],\n",
       "        [ 3.3663]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca3301-dcdb-4e2b-b914-7e6b572eb395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
