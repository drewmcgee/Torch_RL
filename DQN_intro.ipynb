{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ccd5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2a1b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # One input (state), 10 hidden units\n",
    "        self.fc2 = nn.Linear(10, 2)  # Two outputs (Q-values for each action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f189daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote this function (not from ChatGPT)\n",
    "def take_action(state,action):\n",
    "    \n",
    "    state_value = int(state.item())  # Extract the value from 1D tensor \n",
    "    \n",
    "    new_state_value = state_value + action +  (-1 * (1 - action))\n",
    "    new_state_value = min(4,new_state_value)\n",
    "    new_state_value = max(0,new_state_value)\n",
    "    \n",
    "    new_state = torch.tensor([new_state_value], dtype=torch.float32)  # Cast state numerical value 1D tensor\n",
    "    \n",
    "    rewards = [10,10,10,10,25]\n",
    "    reward = rewards[new_state_value]\n",
    "    \n",
    "    done = 1 if new_state_value == 4 else 0\n",
    "    \n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4c05b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "gamma = 0.99  # Discount factor\n",
    "num_episodes = 500\n",
    "epsilon = 0.1 # F0or epsilon-greedy action selection\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = torch.tensor([np.random.randint(5)], dtype=torch.float32)  # Start at a random state\n",
    "    for t in range(100):  # Limit the number of steps per episode\n",
    "        # Select action using epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(2)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # .item(): converts PyTorch tensor to a Python scalar\n",
    "                # [1]: because first slot contains value second slot contains index (action 0 or 1)\n",
    "                # max(0): policy_net output is 1D tensor (vector) with only 1 dim to take the max over (dim 0)\n",
    "                action = policy_net(state).max(0)[1].item()\n",
    "\n",
    "        # Take action and observe next_state, reward, done\n",
    "        next_state, reward, done = take_action(state,action)  # Define according to game dynamics\n",
    "\n",
    "        # Compute Q-value\n",
    "        next_state = torch.tensor([next_state], dtype=torch.float32)\n",
    "        next_state_value = policy_net(next_state).max(0)[0].detach()\n",
    "        expected_q_value = reward + gamma * next_state_value * (1 - done)\n",
    "        \n",
    "        q_value = policy_net(state)[action]\n",
    "\n",
    "        # Compute loss and update policy_net\n",
    "        loss = F.smooth_l1_loss(q_value, expected_q_value)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        if t == 99:\n",
    "            ...\n",
    "            #print('episode')\n",
    "            #print(state)\n",
    "            #print(next_state)\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f848f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1001.3024, 1001.3151], grad_fn=<AddBackward0>) tensor([1001.0605, 1000.9422], grad_fn=<AddBackward0>) tensor([1000.8186, 1000.5692], grad_fn=<AddBackward0>) tensor([1000.5767, 1000.1962], grad_fn=<AddBackward0>) tensor([1000.3348,  999.8232], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "policy_net(torch.tensor([0], dtype=torch.float32)),\n",
    "policy_net(torch.tensor([1], dtype=torch.float32)),\n",
    "policy_net(torch.tensor([2], dtype=torch.float32)),\n",
    "policy_net(torch.tensor([3], dtype=torch.float32)),\n",
    "policy_net(torch.tensor([4], dtype=torch.float32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb66b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8aa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
