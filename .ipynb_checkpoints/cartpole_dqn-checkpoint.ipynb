{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ca16f-7435-4aff-bce0-b739d3dc5dae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44510800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec12eaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeeb500",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train a simple NN on a single training sample!\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 2)  # 784 input features, 128 output features\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.fc3 = nn.Linear(2, 2)\n",
    "        self.fc4 = nn.Linear(2, 1)\n",
    "        #self.fc3 = nn.Linear(2, 1)    # 10 output features for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d705fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Net\n",
    "net = Net()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Optimizer stores the weights / biases\n",
    "\n",
    "# Random data and target labels for demonstration\n",
    "inputs = torch.randn(1, 4)  # Example input\n",
    "targets = torch.tensor([0])   # Example target. know to treat this index of class 0-9 and compare to max output?\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward + backward + optimize\n",
    "outputs = net(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32858cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Now lets try training on multiple samples\n",
    "\n",
    "With multiple samples you can choose how often you want to backpropagate. For example do you want to backprop after every sample or backprop after completing a batch.  Or even after a full pass through the training data (an epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f2d28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552fceb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Random data and target labels for demonstration\n",
    "inputs = torch.randn(10000, 4)  # tensor of 10 vectors of length 4.\n",
    "targets = torch.tensor([[sum(inputs[i])] for i in range(len(inputs))], dtype=torch.float32)  # target is sum of input vector\n",
    "#targets = torch.tensor([[1.1]]*10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a65ccb9",
   "metadata": {},
   "source": [
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward + backward + optimize\n",
    "outputs = net(inputs)\n",
    "#loss = criterion(outputs, targets)\n",
    "loss = F.smooth_l1_loss(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfab40a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "losses = []\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "for i in range(len(inputs) // batch_size + 1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    input_batch = inputs[i: i + batch_size]\n",
    "    target_batch = targets[i: i + batch_size]\n",
    "    \n",
    "\n",
    "    \n",
    "    output_batch = net(input_batch)\n",
    "    loss = F.mse_loss(output_batch, target_batch)\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72aa991",
   "metadata": {},
   "source": [
    "## Throwing minimal DQN at Cartpole-V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b642dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQN Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 24)  # CartPole state is 4-dimensional\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        #self.fc2_1 = nn.Linear(24, 24) \n",
    "        #self.fc2_2 = nn.Linear(24, 24)\n",
    "        #self.fc2_3 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, 2)  # Two actions: push cart left or right\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #x = torch.relu(self.fc2_1(x))\n",
    "        #x = torch.relu(self.fc2_2(x))\n",
    "        #x = torch.relu(self.fc2_3(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acb35a",
   "metadata": {},
   "source": [
    "Below we try our oversimplified DQN on Cartpole. It doesn't learn anything. I think this is because the penalty (negative reward) of the pole falling is only directly attributed to the single action before it and not the series of actions before it, which may have already guaranteed the fall. In this example, we are backpropping after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889feed8",
   "metadata": {},
   "source": [
    "I think what we need to do is rather than backprop after each step we should backprop after each episode and make sure the negative reward is visible to a window of several timesteps before the actual fall. I'm guessing this is where a replay buffer comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Blur the state\n",
    "        precision = 0.001\n",
    "        state_tensor = torch.round(state_tensor / precision) * precision\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -1  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_value = (torch.tensor(reward) + gamma * \n",
    "                                       model(next_state_tensor).max(1)[0] * (1-done)).unsqueeze(0)\n",
    "\n",
    "        # Compute the loss\n",
    "        state_action_value = model(state_tensor)[0, action].unsqueeze(0)\n",
    "        loss = loss_fn(state_action_value, expected_state_action_value)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "    \n",
    "    if t >= 500:\n",
    "        print(\"breaking on 500 training\")\n",
    "        break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d032f-22e0-4976-b1af-02a9b74f3873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Model and Display on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5400709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "\n",
    "def eval_model(yo_model):\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    state = env.reset()[0]\n",
    "    timesteps = 0 \n",
    "    actions = []\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action = yo_model(state_tensor).max(1)[1].item()\n",
    "        actions.append(action)\n",
    "        timesteps += 1\n",
    "        #action = modelenv.action_space.sample()  # replace with your action selection method\n",
    "        state, reward, term,trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            #state = env.reset()[0]\n",
    "            print(\"time steps:\", timesteps)\n",
    "            break\n",
    "            \n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdb58c-f68a-43e0-8c9a-392b20304930",
   "metadata": {},
   "source": [
    "## Control Cartpole Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa69896-6c49-491c-b897-50b57b3fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\",render_mode='human')\n",
    "env.reset()\n",
    "while True:\n",
    "    action = int(input(\"Action: \"))\n",
    "    if action in (0, 1):\n",
    "        state,reward,term,trunc,info = env.step(action)\n",
    "        env.render()\n",
    "        if term or trunc:\n",
    "            break\n",
    "         \n",
    "#env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4819b83-ef9c-4e20-b3bd-3bf568717c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame and the environment\n",
    "pygame.init()\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Set up the display\n",
    "screen_width, screen_height = 600, 400\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption(\"Click Window to Start\")\n",
    "\n",
    "# Start the environment\n",
    "observation = env.reset()\n",
    "done = False\n",
    "clock = pygame.time.Clock()\n",
    "steps = 0\n",
    "\n",
    "waiting_for_click = True\n",
    "while waiting_for_click:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:  # Wait for click to start\n",
    "            waiting_for_click = False\n",
    "        elif event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()  # Exit the entire script if the window is closed\n",
    "\n",
    "\n",
    "while not done or done:\n",
    "    # Render the environment to the pygame window\n",
    "    env.render()\n",
    "\n",
    "    # Check for key presses to control the cart\n",
    "    #action = 1  # Default action (don't move)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_LEFT:\n",
    "                action = 0  # Move cart to the left\n",
    "            elif event.key == pygame.K_RIGHT:\n",
    "                action = 1  # Move cart to the right\n",
    "        elif event.type == pygame.QUIT:\n",
    "            done = True\n",
    "\n",
    "    # Step the environment with the chosen action\n",
    "    observation, reward, term,trunc, info = env.step(action)\n",
    "    steps += 1\n",
    "    done = term or trunc\n",
    "\n",
    "    # Update the display and wait a short duration\n",
    "    pygame.display.flip()\n",
    "    clock.tick(5)  # Limit to 60 frames per second\n",
    "\n",
    "print(\"your timesteps:\", steps)\n",
    "\n",
    "#env.close()\n",
    "#pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0408b4b-0920-4286-b7f8-f9e1d015340e",
   "metadata": {},
   "source": [
    "## Implement a Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdad570-bf2c-4b75-8797-c121b1fed8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()  # Set the target network to evaluation mode\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Blur the state\n",
    "        #precision = 0.001\n",
    "        #state_tensor = torch.round(state_tensor / precision) * precision\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -10  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_value = (torch.tensor(reward) + gamma * \n",
    "                                       target_model(next_state_tensor).max(1)[0] * (1-done)).unsqueeze(0)\n",
    "\n",
    "        # Compute the loss\n",
    "        state_action_value = model(state_tensor)[0, action].unsqueeze(0)\n",
    "        loss = loss_fn(state_action_value, expected_state_action_value)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "    \n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    #if t >= 500:\n",
    "    #    print(\"breaking on 500 training\")\n",
    "    #    break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f06ec-2e69-439e-a4e8-6053363c47f7",
   "metadata": {},
   "source": [
    "## Now Lets Implement a Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18aa1f5-c1ca-4ad0-9428-af2ed8dffbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049aa793-e832-4ede-bf0e-1dfb73f72ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc8d60c-e2fe-4c3e-82f7-baa033dc1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 15 timesteps\n",
      "Episode 100 finished after 15 timesteps\n",
      "Episode 200 finished after 16 timesteps\n",
      "Episode 300 finished after 15 timesteps\n",
      "Episode 400 finished after 12 timesteps\n",
      "Episode 500 finished after 27 timesteps\n",
      "Episode 600 finished after 14 timesteps\n",
      "Episode 700 finished after 39 timesteps\n",
      "Episode 800 finished after 29 timesteps\n",
      "Episode 900 finished after 53 timesteps\n",
      "Episode 1000 finished after 107 timesteps\n",
      "Episode 1100 finished after 103 timesteps\n",
      "Episode 1200 finished after 43 timesteps\n",
      "Episode 1300 finished after 70 timesteps\n",
      "Episode 1400 finished after 23 timesteps\n",
      "Episode 1500 finished after 48 timesteps\n",
      "Episode 1600 finished after 83 timesteps\n",
      "Episode 1700 finished after 99 timesteps\n",
      "Episode 1800 finished after 83 timesteps\n",
      "Episode 1900 finished after 59 timesteps\n",
      "Episode 2000 finished after 48 timesteps\n",
      "Episode 2100 finished after 83 timesteps\n",
      "Episode 2200 finished after 41 timesteps\n",
      "Episode 2300 finished after 37 timesteps\n",
      "Episode 2400 finished after 27 timesteps\n",
      "Episode 2500 finished after 323 timesteps\n",
      "Episode 2600 finished after 53 timesteps\n",
      "Episode 2700 finished after 56 timesteps\n",
      "Episode 2800 finished after 189 timesteps\n",
      "Episode 2900 finished after 33 timesteps\n",
      "Episode 3000 finished after 30 timesteps\n",
      "Episode 3100 finished after 33 timesteps\n",
      "Episode 3200 finished after 328 timesteps\n",
      "Episode 3300 finished after 92 timesteps\n",
      "Episode 3400 finished after 181 timesteps\n",
      "Episode 3500 finished after 175 timesteps\n",
      "Episode 3600 finished after 55 timesteps\n",
      "Episode 3700 finished after 500 timesteps\n",
      "Episode 3800 finished after 500 timesteps\n",
      "Episode 3900 finished after 39 timesteps\n",
      "Episode 4000 finished after 173 timesteps\n",
      "Episode 4100 finished after 226 timesteps\n",
      "Episode 4200 finished after 500 timesteps\n",
      "Episode 4300 finished after 210 timesteps\n",
      "Episode 4400 finished after 20 timesteps\n",
      "Episode 4500 finished after 181 timesteps\n",
      "Episode 4600 finished after 68 timesteps\n",
      "Episode 4700 finished after 35 timesteps\n",
      "Episode 4800 finished after 116 timesteps\n",
      "Episode 4900 finished after 331 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "model = DQN()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 5000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "batch_size = 50\n",
    "\n",
    "# Training Data Analysis\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))  # Decrease epsilon\n",
    "    \n",
    "    for t in range(1, 10000):  # Limit the number of steps per episode\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        # Select and perform an action using epsilon-greedy policy\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).max(1)[1].item()\n",
    "\n",
    "        # Observe new state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward = -1  # Penalize if the pole fell over\n",
    "\n",
    "        # Move to the next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "\n",
    "        experience = (state_tensor, action, reward, next_state_tensor, done)\n",
    "        replay_buffer.push(*experience)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # Random sample from the Replay Buffer\n",
    "            experience_batch = replay_buffer.sample(batch_size)\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*experience_batch)\n",
    "\n",
    "            #print(batch_state)\n",
    "            #print(batch_action)\n",
    "            #print(batch_reward)\n",
    "            #print(batch_next_state)\n",
    "            #print(batch_done)\n",
    "\n",
    "            # Convert batches to PyTorch tensors\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_action = torch.tensor(batch_action, dtype=torch.long)  # Corrected line\n",
    "            batch_reward = torch.tensor(batch_reward, dtype=torch.float)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "            batch_done = torch.tensor(batch_done, dtype=torch.float)\n",
    "\n",
    "\n",
    "            # Model prediction of Q values  \n",
    "            state_q_value = model(batch_state)[:,0]\n",
    "            state_q_value = torch.stack([state_q_value[idx][action_idx] for idx,action_idx in enumerate(batch_action)])\n",
    "            state_q_value = state_q_value.unsqueeze(1)\n",
    "            \n",
    "            # Compute the expected Q values (Bellman Equation)\n",
    "            expected_q_value_batch = (batch_reward + \n",
    "                                      gamma * model(batch_next_state).max(2)[0].squeeze() * (1-batch_done)).unsqueeze(1)\n",
    "            \n",
    "\n",
    "            loss = loss_fn(state_q_value, expected_q_value_batch)\n",
    "    \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2603ed1-43d5-40c1-93ad-2244463626d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_model\u001b[49m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_model' is not defined"
     ]
    }
   ],
   "source": [
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986201e1-134f-41b5-b1df-4c81813b5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
